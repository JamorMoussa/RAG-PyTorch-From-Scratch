{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ac84e1-652a-459c-bc15-f3ab827b639c",
   "metadata": {},
   "source": [
    "# Local RAG from scratch using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c23d5-d062-442e-9731-c5f225879bbd",
   "metadata": {},
   "source": [
    "This project uses PyTorch to build a local **RAG** pipeline from scratch.\n",
    "\n",
    "We're going to build a chat PDF system. That allows you to give any PDF of your choice, i.e. book, article, etc. Then ask any question and get a custom response. \n",
    "\n",
    "Excellent frameworks, such as [LangChain](https://python.langchain.com/v0.2/docs/introduction/) and [LamaIndex](https://docs.llamaindex.ai/en/stable/), facilitate work with LLMs and help build this kind of pipeline. But our goal is to create everything from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c16585-29be-4b59-8a71-77a5f74d8c6f",
   "metadata": {},
   "source": [
    "## 01. Retrieval Autgmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8516b5ff-3f98-4cbd-a51b-9ea2b52ca7c8",
   "metadata": {},
   "source": [
    "### 1.1 What's RAG? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f47a4c-41a2-43ec-adee-d9ecfcea17f8",
   "metadata": {},
   "source": [
    "**RAG** stands for **Retrieval Autgmented Generation**.\n",
    "\n",
    "The paper introduced it as [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/pdf/2005.11401) from Facebook AI research.\n",
    "\n",
    "Let's break **RAG** down:\n",
    "\n",
    "* **Retrieval** - Seeking relevant information given a query within a database and, for example, getting relevant passages from Wikipedia texts when given a question.\n",
    "* **Augmented** - Using the relevant retrieved information to modify the input (prompts) to a generative model (e.g., LLMs).\n",
    "* **Generation** - Generating output given the augmented input; this is done using a generative model, e.g. LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52289c03-9bb5-48f8-8b34-d40c1edf5497",
   "metadata": {},
   "source": [
    "### 1.2 Why RAG is important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91d356-890b-4cd5-90b5-3c5f81a89cae",
   "metadata": {},
   "source": [
    "**LLMs** excel in language modeling, demonstrating a deep understanding of language. They can produce responses by leveraging the text they have been trained on. Although **LLMs** generate good text, it doesn't mean it is factual.\n",
    "\n",
    "So, let's cover why the RAG is essential for LLMs.\n",
    "\n",
    "* **Prevent Hallucination**: LLMs are probabilistic models, which means they can give incorrect information. For some use cases, getting factual information is critical. This is when the **RAG** comes in; it helps with retrieving some facts about the user's query to improve the generation.\n",
    "\n",
    "* **Custom Data**: LLMs are first pre-trained to understand the language, and then fine-tuned to adapt to specific tasks. However, with this method, every time we receive new data we have to train the model again, which is time and money-consuming. **RAG** can help LLMs by providing them with relevant information without needing to be fine-tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a76b4-e483-42b2-abce-a31c4850ad83",
   "metadata": {},
   "source": [
    "### 1.3 Why Local?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b24cb-82ba-44e6-b377-f442edb90392",
   "metadata": {},
   "source": [
    "All the work is done locally. **Why ?**\n",
    "\n",
    "Privacy, Speed, and Cost. Let's break them down in detail: \n",
    "\n",
    "* **Privacy**: locally we don't need to send sensitive data to an **API**, e.g. ChatGPT API.\n",
    "* **Speed**: we won't have to wait for an API downtime. If our hardware running, the pipeline is running.\n",
    "* **Cost**: using APIs costs money for every request you make. if you have your hardware you need to pay nothing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
